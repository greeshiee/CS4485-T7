{"block_file": {"data_exporters/crystal_rogue.py:data_exporter:python:crystal rogue": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom prometheus_client import start_http_server, Gauge, CollectorRegistry, REGISTRY\nimport random\nimport time\nimport pandas as pd\nfrom typing import List\n\n#make sure this reflects prometheus scrape interval\nscrape_interval = 15\nport = 8000\nip_address = '0.0.0.0'\n\ndef create_gauges_from_strings(string_list):\n    \"\"\"\n        takes a list of names and unregisters them from the collectorregistry to prevent duplicate error\n        returns list of gauges with the string_list as the names and an empty string as the description\n    \"\"\"\n    # Unregister existing metrics if they exist\n    for name in string_list:\n        try:\n            REGISTRY.unregister(REGISTRY._names_to_collectors[name])\n        except KeyError:\n            pass\n    \n    # Create and return new gauges\n    return [Gauge(name, \"\") for name in string_list]\n\ndef update_gauges(df, gauge_list):\n    \"\"\"\n        takes dataframe and List of gauges as arguments then sets each value to each gauge\n        sleep() should be set to Prometheus scrape interval\n    \"\"\"\n    for index, row in df.iterrows():\n        for gauge in gauge_list:\n            gauge_name = gauge._name\n            gauge.set(row[gauge_name])\n        print(index)\n        time.sleep(scrape_interval)\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n        Exports data to some source.\n\n        Args:\n            data: The output from the upstream parent block\n            args: The output from any additional upstream blocks (if applicable)\n\n        Output (optional):\n            Optionally return any object and it'll be logged and\n            displayed when inspecting the block run.\n    \"\"\"\n    \n    #print(data)\n\n    df = pd.DataFrame(data)\n\n    #print(df)\n\n    \"\"\"\n    for i in df.columns:\n        print(i)\n    \"\"\"\n\n    gauges = create_gauges_from_strings(df)\n    #print(gauges[1])\n\n    try:\n        server, t = start_http_server(port, addr=ip_address)\n        print(server)\n        print(t)\n        update_gauges(df, gauges)\n    except KeyboardInterrupt:\n        server.shutdown()\n        t.join()\n        print(\"KeyboardInterrupt: ending server\")\n\n    server.shutdown()\n    t.join()\n    print(\"end of program: ending server\")", "file_path": "data_exporters/crystal_rogue.py", "language": "python", "type": "data_exporter", "uuid": "crystal_rogue"}, "data_exporters/dummy_exporter.yaml:data_exporter:yaml:dummy exporter": {"content": "connector_type: dummy\nprint_msg: true\n", "file_path": "data_exporters/dummy_exporter.yaml", "language": "yaml", "type": "data_exporter", "uuid": "dummy_exporter"}, "data_exporters/kafka_exporter.py:data_exporter:python:kafka exporter": {"content": "from team6_package.core import export_dataframe_to_kafka\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_kafka(df, *args, **kwargs):\n    #Exports data to a Kafka topic using team6_package.\n\n    #Use the package function to export data\n    export_dataframe_to_kafka(df)\n", "file_path": "data_exporters/kafka_exporter.py", "language": "python", "type": "data_exporter", "uuid": "kafka_exporter"}, "data_exporters/kafka_producer.yaml:data_exporter:yaml:kafka producer": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:9092\"\ntopic: student-grades\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n", "file_path": "data_exporters/kafka_producer.yaml", "language": "yaml", "type": "data_exporter", "uuid": "kafka_producer"}, "data_exporters/prometheus_exporter.py:data_exporter:python:prometheus exporter": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom prometheus_client import start_http_server, Gauge, CollectorRegistry, REGISTRY\nimport random\nimport time\nimport pandas as pd\nfrom typing import List\n\n#make sure this reflects prometheus scrape interval\nscrape_interval = 15\nport = 8000\nip_address = '0.0.0.0'\n\ndef create_gauges_from_strings(string_list):\n    \"\"\"\n        takes a list of names and unregisters them from the collectorregistry to prevent duplicate error\n        returns list of gauges with the string_list as the names and an empty string as the description\n    \"\"\"\n    # Unregister existing metrics if they exist\n    for name in string_list:\n        try:\n            REGISTRY.unregister(REGISTRY._names_to_collectors[name])\n        except KeyError:\n            pass\n    \n    # Create and return new gauges\n    return [Gauge(name, \"\") for name in string_list]\n\ndef update_gauges(df, gauge_list):\n    \"\"\"\n        takes dataframe and List of gauges as arguments then sets each value to each gauge\n        sleep() should be set to Prometheus scrape interval\n    \"\"\"\n    for index, row in df.iterrows():\n        for gauge in gauge_list:\n            gauge_name = gauge._name\n            gauge.set(row[gauge_name])\n        print(index)\n        time.sleep(scrape_interval)\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n        Exports data to some source.\n\n        Args:\n            data: The output from the upstream parent block\n            args: The output from any additional upstream blocks (if applicable)\n\n        Output (optional):\n            Optionally return any object and it'll be logged and\n            displayed when inspecting the block run.\n    \"\"\"\n\n    df = pd.DataFrame(data)\n\n    \"\"\"\n    for i in df.columns:\n        print(i)\n    \"\"\"\n\n    gauges = create_gauges_from_strings(df)\n    #print(gauges[1])\n\n    try:\n        server, t = start_http_server(port, addr=ip_address)\n        print(server)\n        print(t)\n        update_gauges(df, gauges)\n    except KeyboardInterrupt:\n        server.shutdown()\n        t.join()\n        print(\"KeyboardInterrupt: ending server\")\n\n    server.shutdown()\n    t.join()\n    print(\"end of program: ending server\")", "file_path": "data_exporters/prometheus_exporter.py", "language": "python", "type": "data_exporter", "uuid": "prometheus_exporter"}, "data_exporters/stream_data_exporter.py:data_exporter:python:stream data exporter": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom typing import Dict, List\nfrom team6_package.core import create_kafka_producer\nimport logging\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_sink\nclass CustomSink(BasePythonSink):\n    def init_client(self):\n        # Kafka configuration\n        self.kafka_topic = 'team6_topic'  # Update with your Kafka topic\n        self.bootstrap_servers = 'kafka:9092'  # Adjust if needed (e.g., 'localhost:29092' if running outside Docker)\n\n        # Create Kafka producer using your package function\n        self.producer = create_kafka_producer(self.bootstrap_servers)\n        logging.info(f\"Kafka producer initialized for topic '{self.kafka_topic}' at '{self.bootstrap_servers}'.\")\n\n    def batch_write(self, messages: List[Dict]):\n        for msg in messages:\n            # Handle message format\n            if isinstance(msg, dict) and 'data' in msg:\n                data = msg['data']\n            else:\n                data = msg  # Assume msg is the whole data\n\n            # Send the message to Kafka\n            try:\n                self.producer.send(self.kafka_topic, value=data)\n                logging.info(f\"Sent message to Kafka: {data}\")\n            except Exception as e:\n                logging.error(f\"Error sending message to Kafka: {e}\")\n\n        # Flush the producer to ensure all messages are sent\n        self.producer.flush()\n        logging.info(\"Kafka producer flushed.\")\n\n    def __del__(self):\n        \"\"\"\n        Clean up resources when the sink is destroyed.\n        \"\"\"\n        if hasattr(self, 'producer'):\n            self.producer.close()\n            logging.info(\"Kafka producer closed.\")", "file_path": "data_exporters/stream_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "stream_data_exporter"}, "data_exporters/team6_data_to_kafka.yaml:data_exporter:yaml:team6 data to kafka": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:9092\"\ntopic: team6_topic\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n", "file_path": "data_exporters/team6_data_to_kafka.yaml", "language": "yaml", "type": "data_exporter", "uuid": "team6_data_to_kafka"}, "data_exporters/test2.py:data_exporter:python:test2": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom typing import Callable, Dict, List\nfrom team6_package.core import generate_data, load_schema\nimport time\nimport logging\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\nkafka_config = {\n    'team6_topic',\n    bootstrap_servers='localhost:29092',\n    auto_offset_reset=auto_offset_reset,\n    enable_auto_commit=enable_auto_commit,\n    value_deserializer=value_deserializer,\n    consumer_timeout_ms=consumer_timeout_ms\n}\n\n@streaming_sink\nclass CustomSink(BasePythonSink):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n        self.kafka_topic = 'team6_topic'\n        self.bootstrap_server = 'kafka:29092'\n        kafka_source = KafkaSource(config=kafka_config)\n\n\n    def batch_write(self, messages: List[Dict]):\n        \"\"\"\n        Batch write the messages to the sink.\n\n        For each message, the message format could be one of the following ones:\n        1. message is the whole data to be wirtten into the sink\n        2. message contains the data and metadata with the foramt {\"data\": {...}, \"metadata\": {...}}\n            The data value is the data to be written into the sink. The metadata is used to store\n            extra information that can be used in the write method (e.g. timestamp, index, etc.).\n        \"\"\"\n        for msg in messages:\n            print(msg)\n            pass\n", "file_path": "data_exporters/test2.py", "language": "python", "type": "data_exporter", "uuid": "test2"}, "data_exporters/vast_spellcaster.py:data_exporter:python:vast spellcaster": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom typing import Dict, List\nimport sqlite3\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\n\n@streaming_sink\nclass CustomSink(BasePythonSink):\n    def init_client(self):\n        \"\"\"\n        Initialize the SQLite connection.\n        \"\"\"\n        self.conn = sqlite3.connect('../student_grades.db')  \n        self.cursor = self.conn.cursor()\n\n        # Create the table if it doesn't exist\n        self.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS student_grade (\n                name TEXT, \n                grade CHAR\n            )\n        \"\"\")\n        self.conn.commit()\n\n\n    def batch_write(self, messages: List[Dict]):\n        \"\"\"\n        Batch write the messages to the SQLite DB.\n        \"\"\"\n        try:\n            for msg in messages:\n                # Assuming msg['data'] contains the student name and grade in JSON format\n                data = msg['data']\n                name = data.get('name')\n                grade = data.get('grade')\n\n                if name and grade:\n                    # Insert the name and grade into the student_grade table\n                    self.cursor.execute(\n                        \"INSERT INTO student_grade (name, grade) VALUES (?, ?)\",\n                        (name, grade)\n                    )\n                    print(f'Saving \"{name}\" with grade \"{grade}\" to SQLite')\n                    self.conn.commit()\n        except sqlite3.Error as e:\n            print(f\"SQLite error: {e}\")\n\n    def __del__(self):\n        \"\"\"\n        Close the connection to SQLite when done.\n        \"\"\"\n        self.conn.close()\n\n", "file_path": "data_exporters/vast_spellcaster.py", "language": "python", "type": "data_exporter", "uuid": "vast_spellcaster"}, "data_loaders/data_generator.py:data_loader:python:data generator": {"content": "import io\nimport pandas as pd\nimport requests\nfrom team6_package import generate_data, save_to_csv, load_schema\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    # Load schema from schema templates folder\n    schema = load_schema('/home/src/schemas/schema.json')\n\n    # Generate batch of data according to schema\n    data = generate_data(schema, num_records=5) #num_records is how many records will be generated.\n\n    # Save data as StringIO object\n    csv = save_to_csv(data)\n\n    return pd.read_csv(csv, sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/data_generator.py", "language": "python", "type": "data_loader", "uuid": "data_generator"}, "data_loaders/generator.py:data_loader:python:generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nimport random\nimport time\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\npossible_names = ['Scarlett', 'Melody', 'Bennett', 'Drew', 'Olivia', 'Mariah', 'Jose', 'Levi', 'Nikhil', 'Quinn']\npossible_grades = ['A','B','C','D','E','F']\n\n# NOTE: edit interval_seconds to change how often (in seconds) records are generated\ninterval_seconds = 30\n\n# NOTE: edit batch_size to change how many records are generated per time interval\nbatch_size = 3\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        num_names = len(possible_names)\n        num_grades = len(possible_grades)\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n\n            # Generate random name/grade records\n            for i in range(batch_size):\n                name = possible_names[random.randrange(0,num_names)]\n                grade = possible_grades[random.randrange(0,num_grades)]\n                new_record = {\"data\": {\"name\": name, \"grade\": grade}}\n                records.append(new_record)\n\n            if len(records) > 0:\n                handler(records)\n            \n            # Sleep in between every set of records generated and sent\n            time.sleep(interval_seconds)\n", "file_path": "data_loaders/generator.py", "language": "python", "type": "data_loader", "uuid": "generator"}, "data_loaders/kafka_consumer.yaml:data_loader:yaml:kafka consumer": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:9092\"\ntopic: student-grades\nconsumer_group: unique_consumer_group\ninclude_metadata: true\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n\n# Uncomment the config below to use protobuf schema to deserialize message\n# serde_config:\n#   serialization_method: PROTOBUF\n#   schema_classpath: \"path.to.schema.SchemaClass\"\n", "file_path": "data_loaders/kafka_consumer.yaml", "language": "yaml", "type": "data_loader", "uuid": "kafka_consumer"}, "data_loaders/load_from_kafka.yaml:data_loader:yaml:load from kafka": {"content": "connector_type: kafka\nbootstrap_server: 'localhost:29092'\ntopic: 'team6_topic'\nconsumer_group: unique_consumer_group\ninclude_metadata: false\napi_version: 0.10.2\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n\n# Uncomment the config below to use protobuf schema to deserialize message\n# serde_config:\n#   serialization_method: PROTOBUF\n#   schema_classpath: \"path.to.schema.SchemaClass\"\n", "file_path": "data_loaders/load_from_kafka.yaml", "language": "yaml", "type": "data_loader", "uuid": "load_from_kafka"}, "data_loaders/prometheus_loader.py:data_loader:python:prometheus loader": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom team6_package.core import consume_messages_from_kafka\n\ndef process_message(message):\n    print(f\"Processing message: {message}\")\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n\n    print(kafka_data)\n\n    return 1\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/prometheus_loader.py", "language": "python", "type": "data_loader", "uuid": "prometheus_loader"}, "data_loaders/stream_batch_data_generator.py:data_loader:python:stream batch data generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nfrom team6_package.core import generate_batch_with_time_intervals, load_schema\nimport logging\nfrom datetime import datetime, timedelta\nimport time\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        # Load the schema\n        self.schema = load_schema('/home/src/schemas/schema.json')\n        \n        # Set parameters for data generation\n        self.interval_between_batches = 15  # Time between batches in seconds\n        self.records_per_batch = 1  # Number of records per batch\n        self.record_interval_seconds = 5  # Interval between records in a batch in seconds\n        self.total_batches = None  # Set to None for indefinite streaming\n        self.batch_count = 0\n        self.start_time = None  # Set to specific datetime if needed. If None, uses current time in UTC.\n        self.current_time = self.start_time  # Initialize current_time here\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        logging.info(\"Starting batch_read in CustomSource.\")\n        try:\n            while True:\n                # Generate the batch with time intervals\n                records = generate_batch_with_time_intervals(\n                    self.schema,\n                    self.records_per_batch,\n                    start_time=self.current_time,\n                    interval_seconds=self.record_interval_seconds\n                )\n\n                if records:\n                    handler(records)\n                    self.batch_count += 1\n                    logging.info(f\"Generated and processed batch {self.batch_count} with {len(records)} records.\")\n\n                    # Update self.current_time for the next batch\n                    datetime_field = next(\n                        (key for key in records[-1] if self.schema[key].lower() == 'datetime'), None\n                    )\n                    if datetime_field:\n                        last_record_time_str = records[-1][datetime_field]\n                        self.current_time = datetime.strptime(\n                            last_record_time_str, '%Y-%m-%d %H:%M:%S'\n                        ) + timedelta(seconds=self.record_interval_seconds)\n                    else:\n                        self.current_time = datetime.now()\n                else:\n                    logging.warning(\"No records generated in this batch.\")\n                    self.current_time = datetime.now()\n\n                if self.total_batches is not None and self.batch_count >= self.total_batches:\n                    logging.info(\"Reached total number of batches to send.\")\n                    break\n\n                time.sleep(self.interval_between_batches)\n\n        except Exception as e:\n            logging.error(f\"Error during data streaming: {e}\", exc_info=True)\n        finally:\n            logging.info(\"Data streaming completed.\")", "file_path": "data_loaders/stream_batch_data_generator.py", "language": "python", "type": "data_loader", "uuid": "stream_batch_data_generator"}, "data_loaders/stream_data_generator.py:data_loader:python:stream data generator": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\nfrom team6_package.core import generate_data, load_schema\nimport time\nimport logging\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\nlogging.basicConfig(level=logging.INFO)\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        # Load the schema\n        self.schema = load_schema('/home/src/schemas/schema.json')\n        \n        # Set parameters for data generation\n        self.interval = 15.0  # Interval between batches in seconds\n        self.records_per_batch = 1  # 1 IF YOU WANT A CONSTANT STREAM OF SINGLE RECORDS. FOR MORE THAN 1 USE THE STREAM_BATCH_DATA_GENERATOR INSTEAD\n        self.total_batches = None  # Set to None for indefinite streaming\n        self.batch_count = 0\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        try:\n            while True:\n                records = generate_data(self.schema, self.records_per_batch)\n                if len(records) > 0:\n                    handler(records)\n                    kafka_data = records\n                    print(records)\n                    logging.info(f\"Generated and processed batch {self.batch_count + 1} with {len(records)} records.\")\n                self.batch_count += 1\n                if self.total_batches is not None and self.batch_count >= self.total_batches:\n                    logging.info(\"Reached total number of batches to send.\")\n                    break\n                time.sleep(self.interval)\n        except Exception as e:\n            logging.error(f\"Error during data streaming: {e}\")\n        finally:\n            logging.info(\"Data streaming completed.\")", "file_path": "data_loaders/stream_data_generator.py", "language": "python", "type": "data_loader", "uuid": "stream_data_generator"}, "data_loaders/stream_loader_team6.py:data_loader:python:stream loader team6": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n            if len(records) > 0:\n                handler(records)\n", "file_path": "data_loaders/stream_loader_team6.py", "language": "python", "type": "data_loader", "uuid": "stream_loader_team6"}, "data_loaders/test1.yaml:data_loader:yaml:test1": {"content": "connector_type: kafka\nbootstrap_server: kafka:29092\ntopic: team6_topic\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n\n# Uncomment the config below to use protobuf schema to deserialize message\n# serde_config:\n#   serialization_method: PROTOBUF\n#   schema_classpath: \"path.to.schema.SchemaClass\"\n", "file_path": "data_loaders/test1.yaml", "language": "yaml", "type": "data_loader", "uuid": "test1"}, "data_loaders/test2.py:data_loader:python:test2": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n            if len(records) > 0:\n                handler(records)\n", "file_path": "data_loaders/test2.py", "language": "python", "type": "data_loader", "uuid": "test2"}, "data_loaders/testing1.yaml:data_loader:yaml:testing1": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:29092\"\ntopic: team6_topic\napi_version: 3.8.0\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n\n# Uncomment the config below to use protobuf schema to deserialize message\n# serde_config:\n#   serialization_method: PROTOBUF\n#   schema_classpath: \"path.to.schema.SchemaClass\"\n", "file_path": "data_loaders/testing1.yaml", "language": "yaml", "type": "data_loader", "uuid": "testing1"}, "data_loaders/test_kafka_2.py:data_loader:python:test kafka 2": {"content": "from mage_ai.streaming.sources.base_python import BasePythonSource\nfrom typing import Callable\n\nif 'streaming_source' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_source\n\n\n\n@streaming_source\nclass CustomSource(BasePythonSource):\n    def init_client(self):\n        \"\"\"\n        Implement the logic of initializing the client.\n        \"\"\"\n\n        print(\"hello \", BasePythonSource)\n\n    def batch_read(self, handler: Callable):\n        \"\"\"\n        Batch read the messages from the source and use handler to process the messages.\n        \"\"\"\n        while True:\n            records = []\n            # Implement the logic of fetching the records\n            if len(records) > 0:\n                handler(records)\n", "file_path": "data_loaders/test_kafka_2.py", "language": "python", "type": "data_loader", "uuid": "test_kafka_2"}, "data_loaders/utopian_surf.py:data_loader:python:utopian surf": {"content": "from mage_ai.io.file import FileIO\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_file(*args, **kwargs):\n    \"\"\"\n    Template for loading data from filesystem.\n    Load data from 1 file or multiple file directories.\n\n    For multiple directories, use the following:\n        FileIO().load(file_directories=['dir_1', 'dir_2'])\n\n    Docs: https://docs.mage.ai/design/data-loading#fileio\n    \"\"\"\n    filepath = 'uplink_metrics.csv'\n\n    return FileIO().load(filepath)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/utopian_surf.py", "language": "python", "type": "data_loader", "uuid": "utopian_surf"}, "scratchpads/amazing_druid.py:scratchpad:python:amazing druid": {"content": "import random\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\nstart_timestamp = datetime.now()\nnumRows = 30\n\n#timestamp = []\nuplink_sent_bytes = []\nuplink_received_bytes = []\nuplink_sent_packets = []\nuplink_received_packets = []\nuplink_sent_packets_dropped = []\nuplink_received_packets_dropped = []\n\nfor i in range(numRows):\n    '''\n    timestamp                   (datetime, starting datetime + (i * 5min))\n    sent_bytes                  (integer, received_bytes / 23-25)\n    received_bytes              (integer, 0.1-0.4 gb)\n    sent_packets                (integer, sent_bytes/1000)\n    received_packets            (integer, received_bytes/1000)\n    sent_packets_dropped        (integer, sent_packets * 0-1%)\n    received_packets_dropped    (integer, received_packets * 0-1%)\n    '''\n    #timestamp.append(start_timestamp + timedelta(minutes=i*5))\n    uplink_sent_bytes.append(int(random.uniform(0.1, 0.4) * (1073741824 / random.randint(23,25))))\n    uplink_received_bytes.append(int(random.uniform(0.1, 0.4) * 1073741824))\n    uplink_sent_packets.append(int(uplink_sent_bytes[i] / 1000))\n    uplink_received_packets.append(int(uplink_received_bytes[i] / 1000))\n    uplink_sent_packets_dropped.append(int(random.uniform(0, 0.01) * uplink_sent_packets[i]))\n    uplink_received_packets_dropped.append(int(random.uniform(0, 0.01) * uplink_received_packets[i]))\n\ndf = pd.DataFrame(zip(uplink_sent_bytes, uplink_received_bytes, uplink_sent_packets, uplink_received_packets, uplink_sent_packets_dropped, uplink_received_packets_dropped),\n                  columns=[\"uplink_sent_bytes\", \"uplink_received_bytes\", \"uplink_sent_packets\", \"uplink_received_packets\", \"uplink_sent_packets_dropped\", \"uplink_received_packets_dropped\"])\ndf.to_csv(\"uplink_metrics.csv\", index=False)", "file_path": "scratchpads/amazing_druid.py", "language": "python", "type": "scratchpad", "uuid": "amazing_druid"}, "scratchpads/kpi_testing.py:scratchpad:python:kpi testing": {"content": "\"\"\"\nNOTE: Scratchpad blocks are used only for experimentation and testing out code.\nThe code written here will not be executed as part of the pipeline.\n\"\"\"\nfrom kpi_formula.advanced.data_processor import DataProcessor\nfrom kpi_formula.advanced.data_validator import DataValidator\nfrom kpi_formula.advanced.kpi_calculator import KPICalculator\nfrom kpi_formula.advanced.time_series import TimeSeriesAnalyzer\n\ndef test_all_features():\n    print(\"\\n=== Testing All Features ===\\n\")\n    \n    # \u6d4b\u8bd5\u6570\u636e\n    sales_data = [100, 120, 150, 140, 160, 180, 200, 220, 240, 260, 280, 300,\n                  110, 130, 160, 150, 170, 190, 210, 230, 250, 270, 290, 310]\n    \n    # 1. \u6d4b\u8bd5 DataProcessor\n    print(\"1. DataProcessor Tests:\")\n    ma = DataProcessor.moving_average(sales_data, window=3)\n    print(\"- Moving Average (3 months):\", ma[:5])\n    \n    yoy = DataProcessor.year_over_year_growth(sales_data)\n    print(\"- Year over Year Growth (%):\", [round(x, 2) for x in yoy[:5]])\n    \n    # 2. \u6d4b\u8bd5 DataValidator\n    print(\"\\n2. DataValidator Tests:\")\n    test_data = sales_data + ['invalid', 'error']\n    cleaned_data, errors = DataValidator.validate_numeric(test_data)\n    print(\"- Validation Errors:\", errors)\n    \n    date_test = \"2024-03-20\"\n    is_valid_date = DataValidator.validate_date_format(date_test)\n    print(\"- Date Validation:\", f\"'{date_test}' is valid: {is_valid_date}\")\n    \n    # 3. \u6d4b\u8bd5 KPICalculator\n    print(\"\\n3. KPICalculator Tests:\")\n    roi = KPICalculator.roi(revenue=1000, investment=500)\n    print(\"- ROI (%):\", roi)\n    \n    conv_rate = KPICalculator.conversion_rate(conversions=30, visitors=1000)\n    print(\"- Conversion Rate (%):\", conv_rate)\n    \n    clv = KPICalculator.customer_lifetime_value(\n        avg_purchase_value=100,\n        avg_purchase_frequency=4,\n        customer_lifespan=3\n    )\n    print(\"- Customer Lifetime Value:\", clv)\n    \n    # 4. \u6d4b\u8bd5 TimeSeriesAnalyzer\n    print(\"\\n4. TimeSeriesAnalyzer Tests:\")\n    trend = TimeSeriesAnalyzer.detect_trend(sales_data)\n    print(\"- Trend Direction:\", trend)\n    \n    forecast = TimeSeriesAnalyzer.forecast_simple(sales_data, periods=3)\n    print(\"- Simple Forecast (next 3 periods):\", [round(x, 2) for x in forecast])\n    \n    try:\n        seasonality = TimeSeriesAnalyzer.seasonality(sales_data, period=12)\n        print(\"- Seasonality Analysis:\")\n        print(\"  * Seasonal Factors:\", [round(x, 2) for x in seasonality['seasonal'][:3]])\n        print(\"  * Trend Values:\", [round(x, 2) for x in seasonality['trend'][:3]])\n    except ValueError as e:\n        print(\"- Seasonality Analysis:\", str(e))\n\nif __name__ == \"__main__\":\n    test_all_features()\n\n", "file_path": "scratchpads/kpi_testing.py", "language": "python", "type": "scratchpad", "uuid": "kpi_testing"}, "scratchpads/loader.py:scratchpad:python:loader": {"content": "import json\nfrom confluent_kafka import Consumer, KafkaError\nfrom prometheus_client import start_http_server, Gauge\nimport threading\nimport logging\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Kafka Configuration\nKAFKA_BOOTSTRAP_SERVERS = 'localhost:29092'\nKAFKA_GROUP_ID = 'prometheus-exporter-group'\nKAFKA_TOPIC = 'team6_topic'\n\n# Prometheus Metrics\n# Create Prometheus Gauges to track Kafka message data\n# You can customize these based on your specific message structure\nmessage_count = Gauge('kafka_messages_total', 'Total number of Kafka messages processed')\nmessage_payload = Gauge('kafka_message_payload', 'Kafka message payload', \n                        ['topic', 'key', 'attribute'])\n\nclass KafkaPrometheusExporter:\n    def __init__(self, bootstrap_servers, group_id, topic):\n        \"\"\"\n        Initialize Kafka Consumer and Prometheus Exporter\n        \n        :param bootstrap_servers: Kafka bootstrap servers\n        :param group_id: Kafka consumer group ID\n        :param topic: Kafka topic to consume\n        \"\"\"\n        # Kafka Consumer Configuration\n        self.consumer_config = {\n            'bootstrap.servers': bootstrap_servers,\n            'group.id': group_id,\n            'auto.offset.reset': 'latest'\n        }\n        \n        # Create Kafka Consumer\n        self.consumer = Consumer(self.consumer_config)\n        \n        # Subscribe to the topic\n        self.consumer.subscribe([topic])\n        \n        # Track metrics\n        self.processed_messages = 0\n\n    def consume_messages(self):\n        \"\"\"\n        Consume messages from Kafka and update Prometheus metrics\n        \"\"\"\n        try:\n            while True:\n                # Poll for messages\n                msg = self.consumer.poll(1.0)\n                \n                if msg is None:\n                    continue\n                \n                if msg.error():\n                    if msg.error().code() == KafkaError._PARTITION_EOF:\n                        logger.info('Reached end of partition')\n                    else:\n                        logger.error(f'Error: {msg.error()}')\n                    continue\n                \n                # Process the message\n                try:\n                    # Increment total message count\n                    message_count.inc()\n                    self.processed_messages += 1\n                    \n                    # Parse message value\n                    value = msg.value().decode('utf-8')\n                    try:\n                        parsed_value = json.loads(value)\n                        \n                        # Example of setting gauge with parsed data\n                        # Customize this based on your actual message structure\n                        if isinstance(parsed_value, dict):\n                            for key, val in parsed_value.items():\n                                message_payload.labels(\n                                    topic=msg.topic(), \n                                    key=key, \n                                    attribute=key\n                                ).set(val)\n                    except json.JSONDecodeError:\n                        logger.warning(f'Could not parse message: {value}')\n                    \n                    logger.info(f'Processed message: {value}')\n                \n                except Exception as e:\n                    logger.error(f'Error processing message: {e}')\n        \n        except KeyboardInterrupt:\n            logger.info('Stopping consumer')\n        \n        finally:\n            # Close consumer\n            self.consumer.close()\n\ndef start_prometheus_server(port=8000):\n    \"\"\"\n    Start Prometheus HTTP server for metrics exposure\n    \n    :param port: Port to expose metrics on\n    \"\"\"\n    logger.info(f'Starting Prometheus metrics server on port {port}')\n    start_http_server(port)\n\ndef main():\n    # Create Kafka Prometheus Exporter\n    exporter = KafkaPrometheusExporter(\n        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n        group_id=KAFKA_GROUP_ID,\n        topic=KAFKA_TOPIC\n    )\n    \n    # Start Prometheus metrics server in a separate thread\n    prometheus_thread = threading.Thread(\n        target=start_prometheus_server, \n        daemon=True\n    )\n    prometheus_thread.start()\n    \n    # Start Kafka message consumption\n    exporter.consume_messages()\n\nif __name__ == '__main__':\n    main()", "file_path": "scratchpads/loader.py", "language": "python", "type": "scratchpad", "uuid": "loader"}, "scratchpads/test1.py:scratchpad:python:test1": {"content": "from team6_package.core import consume_messages_from_kafka\n\ndef process_message(message):\n    print(f\"Processing message: {message}\")\n\ndef main():\n    kafka_topic = 'team6_topic'\n    bootstrap_servers = 'localhost:29092'\n\n\nif __name__ == '__main__':\n    main()", "file_path": "scratchpads/test1.py", "language": "python", "type": "scratchpad", "uuid": "test1"}, "transformers/elven_healer.py:transformer:python:elven healer": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/elven_healer.py", "language": "python", "type": "transformer", "uuid": "elven_healer"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/prometheus_transformer.py:transformer:python:prometheus transformer": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/prometheus_transformer.py", "language": "python", "type": "transformer", "uuid": "prometheus_transformer"}, "transformers/test_kafka_3.py:transformer:python:test kafka 3": {"content": "from typing import Dict, List\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform(messages: List[Dict], *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Args:\n        messages: List of messages in the stream.\n\n    Returns:\n        Transformed messages\n    \"\"\"\n    # Specify your transformation logic here\n    var = kwargs['kafka_data']\n    print(var)\n\n\n    return messages\n", "file_path": "transformers/test_kafka_3.py", "language": "python", "type": "transformer", "uuid": "test_kafka_3"}, "pipelines/batch_data_generation/metadata.yaml:pipeline:yaml:batch data generation/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/data_generator.py\n  downstream_blocks:\n  - kafka_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_generator\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_generator\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/kafka_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kafka_exporter\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_generator\n  uuid: kafka_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 04:10:05.012046+00:00'\ndata_integration: null\ndescription: Generate a batch of data and send it to a kafka topic using team 6's\n  package\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Batch_Data_Generation\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: batch_data_generation\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/batch_data_generation/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "batch_data_generation/metadata"}, "pipelines/batch_data_generation/__init__.py:pipeline:python:batch data generation/  init  ": {"content": "", "file_path": "pipelines/batch_data_generation/__init__.py", "language": "python", "type": "pipeline", "uuid": "batch_data_generation/__init__"}, "pipelines/batch_proof_of_concept/metadata.yaml:pipeline:yaml:batch proof of concept/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - kafka_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/kafka_exporter.py\n    file_source:\n      path: data_exporters/kafka_exporter.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kafka_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_generator\n  uuid: kafka_exporter\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: kpi_testing\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: kpi_testing\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-04 15:30:40.241176+00:00'\ndata_integration: null\ndescription: 'proof of concept: generate data with Team 6''s Python module and store\n  in a SQLite DB'\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: batch_proof_of_concept\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: batch_proof_of_concept\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/batch_proof_of_concept/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "batch_proof_of_concept/metadata"}, "pipelines/batch_proof_of_concept/__init__.py:pipeline:python:batch proof of concept/  init  ": {"content": "", "file_path": "pipelines/batch_proof_of_concept/__init__.py", "language": "python", "type": "pipeline", "uuid": "batch_proof_of_concept/__init__"}, "pipelines/generator_to_kafka/metadata.yaml:pipeline:yaml:generator to kafka/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - kafka_producer\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: kafka-producer\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - generator\n  uuid: kafka_producer\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-10-03 16:22:31.406582+00:00'\ndata_integration: null\ndescription: 'proof of concept: produce events to a Kafka broker'\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: generator_to_kafka\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: generator_to_kafka\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/generator_to_kafka/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "generator_to_kafka/metadata"}, "pipelines/generator_to_kafka/__init__.py:pipeline:python:generator to kafka/  init  ": {"content": "", "file_path": "pipelines/generator_to_kafka/__init__.py", "language": "python", "type": "pipeline", "uuid": "generator_to_kafka/__init__"}, "pipelines/kafka_to_sqlite/metadata.yaml:pipeline:yaml:kafka to sqlite/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - vast_spellcaster\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: kafka consumer\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: kafka_consumer\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: vast spellcaster\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - kafka_consumer\n  uuid: vast_spellcaster\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-09-30 18:29:03.618047+00:00'\ndata_integration: null\ndescription: 'proof of concept: consume events from Kafka broker and store in local\n  SQLite DB'\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: kafka_to_sqlite\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: kafka_to_sqlite\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/kafka_to_sqlite/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "kafka_to_sqlite/metadata"}, "pipelines/kafka_to_sqlite/__init__.py:pipeline:python:kafka to sqlite/  init  ": {"content": "", "file_path": "pipelines/kafka_to_sqlite/__init__.py", "language": "python", "type": "pipeline", "uuid": "kafka_to_sqlite/__init__"}, "pipelines/original_prometheus_generator/metadata.yaml:pipeline:yaml:original prometheus generator/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: amazing druid\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: amazing_druid\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - elven_healer\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: utopian surf\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: utopian_surf\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - crystal_rogue\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: elven healer\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - utopian_surf\n  uuid: elven_healer\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: crystal rogue\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - elven_healer\n  uuid: crystal_rogue\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-22 19:31:15.560189+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: original_prometheus_generator\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: original_prometheus_generator\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/original_prometheus_generator/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "original_prometheus_generator/metadata"}, "pipelines/original_prometheus_generator/__init__.py:pipeline:python:original prometheus generator/  init  ": {"content": "", "file_path": "pipelines/original_prometheus_generator/__init__.py", "language": "python", "type": "pipeline", "uuid": "original_prometheus_generator/__init__"}, "pipelines/prometheus_payload/metadata.yaml:pipeline:yaml:prometheus payload/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: loader\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-12-02 19:58:41.866878+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: prometheus_payload\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: prometheus_payload\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/prometheus_payload/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "prometheus_payload/metadata"}, "pipelines/prometheus_payload/__init__.py:pipeline:python:prometheus payload/  init  ": {"content": "", "file_path": "pipelines/prometheus_payload/__init__.py", "language": "python", "type": "pipeline", "uuid": "prometheus_payload/__init__"}, "pipelines/stream_data_generator/metadata.yaml:pipeline:yaml:stream data generator/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/stream_data_generator.py\n  downstream_blocks:\n  - team6_data_to_kafka\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: stream_data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: stream_data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: team6_data_to_kafka\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - stream_data_generator\n  uuid: team6_data_to_kafka\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 04:48:48.602145+00:00'\ndata_integration: null\ndescription: generate a stream (single or batch) of data and export it to a kafka\n  topic.\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: stream_data_generator\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: stream_data_generator\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/stream_data_generator/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "stream_data_generator/metadata"}, "pipelines/stream_data_generator/__init__.py:pipeline:python:stream data generator/  init  ": {"content": "", "file_path": "pipelines/stream_data_generator/__init__.py", "language": "python", "type": "pipeline", "uuid": "stream_data_generator/__init__"}, "pipelines/stream_of_batches_data_generator/metadata.yaml:pipeline:yaml:stream of batches data generator/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - team6_data_to_kafka\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: stream_batch_data_generator\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: stream_batch_data_generator\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/team6_data_to_kafka.yaml\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: team6_data_to_kafka\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - stream_batch_data_generator\n  uuid: team6_data_to_kafka\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-14 06:00:13.109679+00:00'\ndata_integration: null\ndescription: generate a stream of batches and send them to kafka topic using team\n  6's package\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: stream_of_batches_data_generator\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: stream_of_batches_data_generator\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/stream_of_batches_data_generator/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "stream_of_batches_data_generator/metadata"}, "pipelines/stream_of_batches_data_generator/__init__.py:pipeline:python:stream of batches data generator/  init  ": {"content": "", "file_path": "pipelines/stream_of_batches_data_generator/__init__.py", "language": "python", "type": "pipeline", "uuid": "stream_of_batches_data_generator/__init__"}, "pipelines/test/metadata.yaml:pipeline:yaml:test/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test1\n  retry_config: null\n  status: updated\n  timeout: null\n  type: scratchpad\n  upstream_blocks: []\n  uuid: test1\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-11-27 02:08:14.133824+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: test\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/test/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test/metadata"}, "pipelines/test/__init__.py:pipeline:python:test/  init  ": {"content": "", "file_path": "pipelines/test/__init__.py", "language": "python", "type": "pipeline", "uuid": "test/__init__"}, "pipelines/test_stream/metadata.yaml:pipeline:yaml:test stream/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - test2\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test1\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test1\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test2\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - test1\n  uuid: test2\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-12-01 02:21:05.253042+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test_stream\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: test_stream\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/test_stream/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test_stream/metadata"}, "pipelines/test_stream/__init__.py:pipeline:python:test stream/  init  ": {"content": "", "file_path": "pipelines/test_stream/__init__.py", "language": "python", "type": "pipeline", "uuid": "test_stream/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}